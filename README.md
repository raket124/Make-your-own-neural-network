# Make your own neural network

This repository is inspired by the book [Make your own neural network](https://www.goodreads.com/book/show/29746976-make-your-own-neural-network).
It takes the well known [MNIST dataset](http://yann.lecun.com/exdb/mnist/) of handwriten numbers (60.000 training and 10.000 test samples) and uses them to train and validate a fully connected feed-forward neural network.
The experiments show the effects of training the network with different parameters and their respective gains.

The goal for this repository is: Reproduction of the experiments without us IPython.

## Tools & Libraries
- Vagrant (Virtual machine provider)
- Python (Programming language)
  - Numpy (Math library)
  - Matplotlib (Data visualization library)
- FFmpeg + ImageMagic (Encoders)
- Atom (IDE)

## Experiments
The code is split into five experiments and each experiment in three sections.
A main function allows these to be called individually and their results are stored as binaries or images.
The core of the experiments are the [DataPreparer](https://github.com/raket124/Make-your-own-neural-network/blob/master/Code/DataPreparer.py) and the [NeuralNetwork](https://github.com/raket124/Make-your-own-neural-network/blob/master/Code/NeuralNetwork.py) class.
The DataPreparer reads the dataset and returns each individual records as input for the neural network.
The NeuralNetwork represents the feed-forward neural network, which consists of 784 input nodes, 100 hidden nodes and 10 output nodes.
The activation function is the sigmoid and the initial weights are generated by sampling random values from a normal distribution.

### Experiment 1 - A training run with fixed values.
The first experiment is a single training run with a fixed learning rate.
All training images are shown to the network once with a learning rate of 0.3.

### Experiment 2 - The influence of the learning rate
The second experiment trains the neural network with different learning rates.
The expected behaviour is that a small learning rate will prevent the neural network from achieving a proper generic answer.
While a large learning rate will cause the neural network to overshoot the correct answer.

![lr]


### Experiment 3 - Training multiple rounds

![ep]

### Experiment 4 - Learning rates vs Epochs

![lrep]

### Experiment 5 - Inverse query (estimation)


[lr]: https://github.com/raket124/Make-your-own-neural-network/blob/master/Code/Output/LearningRate.png "Learning rate plot"
[ep]: https://github.com/raket124/Make-your-own-neural-network/blob/master/Code/Output/Epoch.png "Epoch plot"
[lrep]: https://github.com/raket124/Make-your-own-neural-network/blob/master/Code/Output/EpochAndLearningRate.gif "Epoch vs learning rate plot"





